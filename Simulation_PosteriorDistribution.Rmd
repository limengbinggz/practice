---
author: "Mengbing Li"
title: "Simulation_PosteriorDistribution"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = 'E:/UM/Research/BayesianDataAnalysis_AndrewGelman')

library(knitr)
library(dplyr)
library(LearnBayes)
```

# Section 3.7 Example: analysis of a bioassay experiement

```{r results='asis'}
dose <- c(-0.86, -0.30, -0.05, 0.73)
deaths <- c(0,1,3,5)
n <- rep(5,4)
data <- as.data.frame(cbind(dose, deaths, n))
colnames(data) <- c("Dose, x_i (log g/ml)", "Number of animals, n_i", "Number of deaths, y_i")
kable(data, caption = "Data from the bioassay experiment")
```

Let $(x_i, n_i, y_i), i=1, \ldots, k$ represent the data, where $x_i =$ the $i$th of $k$ dose levels measured on a log scale given to $n_i$ animals, of which $y_i$ subsequently have positive outcome. Within each group $i$, we assume the outcomes of the five animals are exchangeable and assume they are independent with equal probabilities. This implies that the data points 
$$y_i \mid \theta_i \sim \mathrm{Binomial} (n_i, \theta_i),$$ 
where $\theta_i =$ the probability of death for animals given dose $x_i$. 

The simpliest and reasonable model o the dose-response relation is a simple logistic model, i.e.
$$ \mathrm{logit} (\theta_i) = \beta_0 + \beta_1 x_i.$$
The joint posterior distribution is 
$$
\begin{aligned}
p(\beta_0, \beta_1 \mid y,n,x) &\propto p(\beta_0, \beta_1 \mid n,x) p(y \mid \beta_0, \beta_1, n,x) \\
&= p(\beta_0, \beta_1 \mid n,x) \prod\limits_{i=1}^k p(y_i \mid \beta_0, \beta_1, n_i, x_i) \\
&= p(\beta_0, \beta_1 \mid n,x) \prod\limits_{i=1}^k \left[ \mathrm{logit}^{-1} (\beta_0 + \beta_1 x_i) \right]^{y_i} \left[ 1 - \mathrm{logit}^{-1} (\beta_0 + \beta_1 x_i) \right]^{n_i - y_i}.
\end{aligned}
$$

#### A rough estimate of the parameters

##### Linear regression of logit$(y_i/n_i)$ on $x_i$.
```{r}
y_modified <- c(0.5,1,3,4.5)
data <- data %>% mutate(logit = log((y_modified/n) / (1-(y_modified/n))))
model1 <- lm(data = data, logit~dose)
summary(model1)
```

##### Logistic regression
```{r}
response <- cbind(deaths, n-deaths)
model2 <- glm(data = data, response~dose, family = "binomial")
summary(model2)
```


To estimate the posterior, suppose we have prior beliefs about the regression parameters as the following. First suppose we believe that at a low does level $x_L = -0.7$, the median and 90th percentile of the probability of death $p_L$ are $0.2$ and $0.5$ respectively. Second suppose we believe that at a high dose level $x_H = 0.6$, the same percentiles of the probability of death $p_H$ are $0.8$ and $0.98$ respectively. We may choose a Beta distribution as the conjugate prior. In these two cases, the parameters for the Beta distributions are:

```{r}
beta.select(list(p=0.5, x=0.2), list(p=0.9, x=0.5))

beta.select(list(p=0.5, x=0.8), list(p=0.9, x=0.98))
```

This means that the first prior information matches a Beta$(1.12, 3.56)$ distribution, and the second prior information matches a Beta$(2.10, 0.74)$ distribution. If further the beliefs about the probabilities $p_L$ and $p_H$ are independent, then the joint prior of $(p_L, p_H)$ is given by 
$$ g(p_L, p_H) \propto p_L^{1.12-1} (1-p_L)^{3.56-1} p_H^{2.10-1} (1-p_H)^{0.74-1}. $$

A more general expression of the posterior is already given above. 

Intuitively, the first parameter of the prior Beta distribution can be viewed as the number of successes, and the second parameter can be viewed as the number of failures. Hence the sum of the two parameters can be viewed as the total sample size. 

#### Contour plot of the log posterior density for $(\beta_0, \beta_1)$:
```{r}
prior <- rbind(c(-0.7, 3.56+1.12, 1.12),
               c(0.6, 2.84, 2.10))
data.new <- data[,c("dose", "n", "deaths")]
colnames(prior) <- colnames(data.new)
# data$logit <- NULL
data.new <- rbind(data.new, prior)
mycontour(logisticpost, c(-2,3,-1,12), data.new,
          xlab="beta0", ylab="beta1")

# Simulation from the posterior distribution
s <- simcontour(logisticpost, c(-2,3,-1,11), data.new, 1000)
points(s)
```


#### Plot the density of $\beta_1$ simulated from the posterior distribution
```{r}
plot(density(s$y), xlab = "beta1")
```


#### Estimate LD-50
LD-w is defined to be the x value that satisfies 
$$ g(\mu) = \mathrm{logit}\left\{ \pi \right\} = \beta_0 + \beta_1 w. $$
When $\pi = 0.5$, $w = -\frac{\beta_0}{\beta_1}$.

```{r}
theta <- -s$x / s$y
hist(theta, xlab = "LD-50", breaks = 20)
```

A 95\% credible interval for LD-50 is 
```{r}
quantile(theta, c(0.025, 0.975))
```


In our context, LD-50 is meaningless if $\beta_1 \leq 0$, in which case increasing dose does not increase the probability of death. If we believe that increasing the dose does not decrease the probability of death, we should constrain $\beta_1 > 0$. But putting no such constraint allows for easier estimation. However, note that from the histogrm for $\beta_1$, almost all draws of $\beta_1$ are positive, which means we may simply ignore the constraint in this case.














